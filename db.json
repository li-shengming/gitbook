{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/maupassant/source/css/default.css","path":"css/default.css","modified":0,"renderable":1},{"_id":"themes/maupassant/source/css/donate.css","path":"css/donate.css","modified":0,"renderable":1},{"_id":"themes/maupassant/source/css/style.scss","path":"css/style.scss","modified":0,"renderable":1},{"_id":"themes/maupassant/source/donate/index.html","path":"donate/index.html","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","path":"js/codeblock-resizer.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/donate.js","path":"js/donate.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/fancybox.js","path":"js/fancybox.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/share.js","path":"js/share.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/totop.js","path":"js/totop.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/smartresize.js","path":"js/smartresize.js","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/AliPayQR.png","path":"img/AliPayQR.png","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/BTCQR.png","path":"img/BTCQR.png","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/alipay.svg","path":"img/alipay.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/WeChatQR.png","path":"img/WeChatQR.png","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/bitcoin.svg","path":"img/bitcoin.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/github.svg","path":"img/github.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/like.svg","path":"img/like.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/paypal.svg","path":"img/paypal.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/img/wechat.svg","path":"img/wechat.svg","modified":0,"renderable":1},{"_id":"themes/maupassant/source/js/gitment.browser.js","path":"js/gitment.browser.js","modified":0,"renderable":1}],"Cache":[{"_id":"themes/maupassant/.gitignore","hash":"d7d27e5a9bcffe7f90dc2f4f0752e19020e40f40","modified":1556845563509},{"_id":"themes/maupassant/.travis.yml","hash":"f8da426b97088e4caa5226cff219a5d95087961f","modified":1556845563509},{"_id":"themes/maupassant/LICENSE","hash":"0663fd3a7ea9fc4f4c634b4d73e2da426b536f86","modified":1556845563509},{"_id":"themes/maupassant/_config.yml","hash":"9f9a4b39fffb7aad585d8a92cb7a43ab172107db","modified":1556872035743},{"_id":"themes/maupassant/README.md","hash":"6fe6a7a60257052fd5c0f925241559e30808ab4a","modified":1556845563510},{"_id":"themes/maupassant/package.json","hash":"5328c4c435cd4a5fe47caae31b9975013cdca5bb","modified":1556845563522},{"_id":"source/_posts/hello-world.md","hash":"0876c6dd88b67558b407ff950c74d060568ee7c1","modified":1556869123553},{"_id":"source/_posts/eureka-study.md","hash":"ee153a6bee9f331606580412e312f2ab773f4abe","modified":1556869060732},{"_id":"themes/maupassant/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1556845563496},{"_id":"themes/maupassant/.git/config","hash":"3dbf80769093e21ccab51f5f2b02a6d764498803","modified":1556845563500},{"_id":"source/_posts/redis-study.md","hash":"d85fb88fb8bef373b20b1c7b4e450214d20b293e","modified":1556871487651},{"_id":"source/about/index.md","hash":"3e3d697c1b8740fb59f57588a96bbcef8e0e890b","modified":1556848480439},{"_id":"themes/maupassant/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1556845558725},{"_id":"themes/maupassant/.git/index","hash":"402fa7c322dbcc218c7cd690cde0e9bf56ebeac6","modified":1556871996179},{"_id":"themes/maupassant/.git/packed-refs","hash":"936fbde790cc520205e1157e5102310c9e4bdd49","modified":1556845563493},{"_id":"themes/maupassant/languages/de-DE.yml","hash":"25d1d8cd8113045a7603c14af1ea1539fc6456ed","modified":1556845563512},{"_id":"themes/maupassant/languages/en.yml","hash":"518beaa8538a772ca697122264d667059797e458","modified":1556845563512},{"_id":"themes/maupassant/languages/es-ES.yml","hash":"3cc9312fbdba4a8f8e8254804121e4724c719bcc","modified":1556845563512},{"_id":"themes/maupassant/languages/fr-FR.yml","hash":"3a50568f200b9c1258415b53727e42c6b6c7ea0b","modified":1556845563512},{"_id":"themes/maupassant/languages/ko.yml","hash":"a454bcec60113507bc1d593a699849822386c196","modified":1556845563513},{"_id":"themes/maupassant/languages/ru.yml","hash":"36edc014c6aaef367d58929089bf7915375e71a6","modified":1556845563513},{"_id":"themes/maupassant/languages/zh-CN.yml","hash":"a1a9888b6cd0fd3dc45ffed3490f4ca8ce1abfd7","modified":1556845563513},{"_id":"themes/maupassant/languages/zh-TW.yml","hash":"34dba7ac67aeb316f629ca73e546fa143cc362d5","modified":1556845563514},{"_id":"themes/maupassant/layout/archive.pug","hash":"05f751cb766616ff96ff22ad0790d9cd28777270","modified":1556845563519},{"_id":"themes/maupassant/layout/base.pug","hash":"8d890a05672044dec62d878b8bb279a892be2f2f","modified":1556872828782},{"_id":"themes/maupassant/layout/page.pug","hash":"a21e638d5459120d88e45e8f18a23dc072d9ca07","modified":1556845563521},{"_id":"themes/maupassant/layout/base-without-sidebar.pug","hash":"b0a0ec63ee0225eaa0996d72164202bc9a28a225","modified":1556845563519},{"_id":"themes/maupassant/layout/index.pug","hash":"d1d56fa62f1293cd0cbefe96dbbde2d35b557a0a","modified":1556845563520},{"_id":"themes/maupassant/layout/post.pug","hash":"b112011fbb6a32b33e341be7edf043bc4ad5f97e","modified":1556845563521},{"_id":"themes/maupassant/layout/single-column.pug","hash":"8b4b731cdf86379d526821a1fa950bf15ed61f15","modified":1556845563521},{"_id":"themes/maupassant/layout/timeline.pug","hash":"04f7efdc45acda1faff409d0f80fa5a0dd2309d0","modified":1556845563521},{"_id":"themes/maupassant/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1556845558725},{"_id":"themes/maupassant/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1556845558726},{"_id":"themes/maupassant/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1556845558727},{"_id":"themes/maupassant/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1556845558727},{"_id":"themes/maupassant/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1556845558728},{"_id":"themes/maupassant/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1556845558729},{"_id":"themes/maupassant/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1556845558729},{"_id":"themes/maupassant/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1556845558735},{"_id":"themes/maupassant/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1556845558736},{"_id":"themes/maupassant/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1556845558736},{"_id":"themes/maupassant/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1556845558737},{"_id":"themes/maupassant/.git/logs/HEAD","hash":"543c9893f67a62b4aacb5ebffd64220ae5cebcdd","modified":1556845563498},{"_id":"themes/maupassant/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1556845558737},{"_id":"themes/maupassant/layout/_partial/after_footer.pug","hash":"d2ffd7c34c743ec3c01dc1a1ddb8913ffd025f2a","modified":1556845563514},{"_id":"themes/maupassant/layout/_partial/comments.pug","hash":"efaf0788b6b70aa03c928137cd13b28afa706f75","modified":1556845563514},{"_id":"themes/maupassant/layout/_partial/footer.pug","hash":"92aa15e813bfb411803cc54218feb5410469a9c2","modified":1556845563515},{"_id":"themes/maupassant/layout/_partial/head.pug","hash":"235660024477ba0f68dacbbde12492280f82242a","modified":1556845563515},{"_id":"themes/maupassant/layout/_partial/helpers.pug","hash":"9e44f6d32f2449b4109c33118f8285fa2fc7b023","modified":1556845563515},{"_id":"themes/maupassant/layout/_partial/mathjax.pug","hash":"ac6e3a92bf18ab6bbd0e041b6796b295bae963ee","modified":1556845563515},{"_id":"themes/maupassant/layout/_partial/mathjax2.pug","hash":"234a792e64ba208fa97d2f99772ece23056a53ec","modified":1556845563516},{"_id":"themes/maupassant/layout/_partial/paginator.pug","hash":"03ad0c49ae6f8a999ae35b38d08e25775f51f52a","modified":1556845563516},{"_id":"themes/maupassant/layout/_partial/post_nav.pug","hash":"b11d9e6000449838b17f508429f29ffb60f53096","modified":1556845563516},{"_id":"themes/maupassant/layout/_partial/tag.pug","hash":"6145b483b271bba05ad1db7c039fe352a768215b","modified":1556845563517},{"_id":"themes/maupassant/layout/_partial/totop.pug","hash":"eb91a3baf9411188c7c8130f63a674f541ca9c81","modified":1556845563517},{"_id":"themes/maupassant/layout/_partial/wordcount.pug","hash":"6fb1d8ee09cfc4fd31e8ae53736e4f479d45e31e","modified":1556845563517},{"_id":"themes/maupassant/layout/_widget/category.pug","hash":"7707b4c718a935882ee986d0bb0078e50cdbea64","modified":1556845563517},{"_id":"themes/maupassant/layout/_widget/links.pug","hash":"7bc7c17cfd498c3e0c3371cef78f08f1dc25db36","modified":1556845563518},{"_id":"themes/maupassant/layout/_widget/recent_comments.pug","hash":"68bae3eb2f80e6127e03faa7ee1b78fb2e70aafc","modified":1556845563518},{"_id":"themes/maupassant/layout/_widget/recent_posts.pug","hash":"770b6c41cbf7969ed33adf87eec3be6f50a0911b","modified":1556845563518},{"_id":"themes/maupassant/layout/_widget/search.pug","hash":"6aa743486f282545f553a4fad6aae037afe26108","modified":1556845563518},{"_id":"themes/maupassant/layout/_widget/tag.pug","hash":"37f236365b153fc40324391e5a602d6d50014e18","modified":1556845563519},{"_id":"themes/maupassant/layout/_widget/toc.pug","hash":"0914d76c406067ee1a00540c89ce0dd9473f5a35","modified":1556872417121},{"_id":"themes/maupassant/source/css/default.css","hash":"b41d95120f9e64fd4530ae00ceaef09c7ea20818","modified":1556845563522},{"_id":"themes/maupassant/source/css/donate.css","hash":"ca39d14a598e1de5c51db4170ceb399c6a7131fe","modified":1556845563522},{"_id":"themes/maupassant/source/css/style.scss","hash":"ab4877f2100405b2c1c161adf2d56a15347e4ce1","modified":1556872910554},{"_id":"themes/maupassant/source/donate/index.html","hash":"72e48cbc939df9e4f963ebf570e0181e129a51c5","modified":1556845563523},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","hash":"c77270e684a60babc1abb7353e700ecdc5a66d30","modified":1556845563536},{"_id":"themes/maupassant/source/js/donate.js","hash":"780beaaf44b1e6c057752bdbc085b1048937e5e7","modified":1556845563536},{"_id":"themes/maupassant/source/js/fancybox.js","hash":"8a993c1c4ad40789d2960b682cb2130382a0f26a","modified":1556845563536},{"_id":"themes/maupassant/source/js/search.js","hash":"dbda07a03e6edc73f1dc28a068c24a6037b97b56","modified":1556845563538},{"_id":"themes/maupassant/source/js/share.js","hash":"514e726c1efae9f6566600fa0e945b4b9e620f2e","modified":1556845563538},{"_id":"themes/maupassant/source/js/totop.js","hash":"15de186b089c245fe60766d509b587919f05ff23","modified":1556845563539},{"_id":"themes/maupassant/source/js/smartresize.js","hash":"150ab1cad40d7ae081b0896b13f7d7cbac4e6338","modified":1556845563539},{"_id":"themes/maupassant/source/img/AliPayQR.png","hash":"7787b5d91cbf0e19a1260df24f7d949771c7d45b","modified":1556845563524},{"_id":"themes/maupassant/source/img/BTCQR.png","hash":"7d1c80f953bfb6f0a37d432b04c936ea165bfd97","modified":1556845563524},{"_id":"themes/maupassant/source/img/alipay.svg","hash":"292ea040e865c1d0be259703ff850570b3bdfc97","modified":1556845563525},{"_id":"themes/maupassant/source/img/WeChatQR.png","hash":"8c41aca7883e5ff714c56556f5fff8e7e7c38093","modified":1556845563524},{"_id":"themes/maupassant/source/img/bitcoin.svg","hash":"eeb2ee8cf44ba5c298baeed84bb90866f4814955","modified":1556845563525},{"_id":"themes/maupassant/source/img/github.svg","hash":"90ba9a3b0dc19e70e742a39b014194f801e00f97","modified":1556845563525},{"_id":"themes/maupassant/source/img/like.svg","hash":"22a2754dc454d7b0321b70914fb2936b8d2ea8ab","modified":1556845563535},{"_id":"themes/maupassant/source/img/paypal.svg","hash":"e916dea1c1bba1bc935510310f65b2c9328a401a","modified":1556845563535},{"_id":"themes/maupassant/source/img/wechat.svg","hash":"30418295bed44bcc4b29076eb7deed49cf4d6c1c","modified":1556845563535},{"_id":"themes/maupassant/.git/objects/23/36cc26e7aabf63b055a8e2838f944929dd1834","hash":"2b9213eb92bb61c006c71b29ff6ad8511a0fe0b5","modified":1556871996162},{"_id":"themes/maupassant/.git/objects/f4/0283c2bc79dd7c2a425a72dbf41f03cae5ec67","hash":"81f0d34bd2271644d952af55d055821eec276dbe","modified":1556871996123},{"_id":"themes/maupassant/.git/objects/pack/pack-8001fff696543c3a0ab8e15c48728d3f1745fe24.idx","hash":"795e56b8cf1ca88964dd4edd1f5586580d7fb02c","modified":1556845563339},{"_id":"themes/maupassant/.git/refs/heads/master","hash":"aba579e16aee2b1085818c7208dc13bd3dff2601","modified":1556845563497},{"_id":"themes/maupassant/source/js/gitment.browser.js","hash":"b15998a45d5f386d30905cfbfbb1658336acbb5b","modified":1556845563537},{"_id":"themes/maupassant/.git/logs/refs/heads/master","hash":"543c9893f67a62b4aacb5ebffd64220ae5cebcdd","modified":1556845563498},{"_id":"themes/maupassant/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1556845563496},{"_id":"themes/maupassant/.git/logs/refs/remotes/origin/HEAD","hash":"543c9893f67a62b4aacb5ebffd64220ae5cebcdd","modified":1556845563495},{"_id":"themes/maupassant/.git/objects/pack/pack-8001fff696543c3a0ab8e15c48728d3f1745fe24.pack","hash":"5ece2f8c91c9796e77719836f6ed06c10f5ab64d","modified":1556871927000},{"_id":"public/search.xml","hash":"7be03d01917b94a171872f5acd0bb376c2b7285a","modified":1556872959252},{"_id":"public/about/index.html","hash":"93001347c44c04cf0d9a1f87d3c49b8f10b512bb","modified":1556872959255},{"_id":"public/2019/05/02/eureka-study/index.html","hash":"acff8a6fe56c661f4cb0e68d657fc7d4985b19d3","modified":1556872959255},{"_id":"public/2019/05/01/hello-world/index.html","hash":"1cd432ff123d7c003b2c40fc5fbefcbd4a43a4c6","modified":1556872959256},{"_id":"public/index.html","hash":"7a9942917535fcaebeffe24ca95a30c9d5969a89","modified":1556872959256},{"_id":"public/archives/index.html","hash":"2b0643fdafcf560b5d4d9961fb32c61119fc6cac","modified":1556872959256},{"_id":"public/archives/2019/index.html","hash":"2b0643fdafcf560b5d4d9961fb32c61119fc6cac","modified":1556872959256},{"_id":"public/archives/2019/05/index.html","hash":"2b0643fdafcf560b5d4d9961fb32c61119fc6cac","modified":1556872959256},{"_id":"public/categories/SpringCloud/index.html","hash":"c9e1463422f625ad25d71037298c06a0b15a8b10","modified":1556872959256},{"_id":"public/categories/DB/index.html","hash":"127b2056b229a492ded5b25325344443b9aec5da","modified":1556872959256},{"_id":"public/tags/Eureka/index.html","hash":"6ee835eb099cd056da5e05fb1801d80793303440","modified":1556872959256},{"_id":"public/tags/SourceCode/index.html","hash":"17c34b2af8edaee231b5406112d60e84dbc66fb0","modified":1556872959256},{"_id":"public/tags/Redis/index.html","hash":"bb5e9cf7077f9b06e2c72bbe3e2923ff43619d82","modified":1556872959256},{"_id":"public/2019/05/03/redis-study/index.html","hash":"d7498a3b72b0e4346145e73e846a91a6b217132d","modified":1556872959256},{"_id":"public/img/github.svg","hash":"90ba9a3b0dc19e70e742a39b014194f801e00f97","modified":1556872959258},{"_id":"public/img/AliPayQR.png","hash":"7787b5d91cbf0e19a1260df24f7d949771c7d45b","modified":1556872959258},{"_id":"public/img/bitcoin.svg","hash":"eeb2ee8cf44ba5c298baeed84bb90866f4814955","modified":1556872959258},{"_id":"public/img/alipay.svg","hash":"292ea040e865c1d0be259703ff850570b3bdfc97","modified":1556872959258},{"_id":"public/img/BTCQR.png","hash":"7d1c80f953bfb6f0a37d432b04c936ea165bfd97","modified":1556872959259},{"_id":"public/img/wechat.svg","hash":"30418295bed44bcc4b29076eb7deed49cf4d6c1c","modified":1556872959259},{"_id":"public/img/like.svg","hash":"22a2754dc454d7b0321b70914fb2936b8d2ea8ab","modified":1556872959259},{"_id":"public/img/paypal.svg","hash":"e916dea1c1bba1bc935510310f65b2c9328a401a","modified":1556872959259},{"_id":"public/img/WeChatQR.png","hash":"8c41aca7883e5ff714c56556f5fff8e7e7c38093","modified":1556872959259},{"_id":"public/css/donate.css","hash":"f019876946aeb80e567ece250d54c1327c794583","modified":1556872959263},{"_id":"public/js/share.js","hash":"a2f9de374523dc7f2ddb90ed5f24b668c20d9272","modified":1556872959263},{"_id":"public/js/fancybox.js","hash":"13c4781570339f4fba76a3d7f202e442817dd605","modified":1556872959263},{"_id":"public/js/donate.js","hash":"89f0b9d9d0c4fce183161d29c2a44aef750efb27","modified":1556872959263},{"_id":"public/js/totop.js","hash":"7dbf8fcf582a4fb6eb9b2c60d6de9f9c2091ec4c","modified":1556872959264},{"_id":"public/js/smartresize.js","hash":"3ef157fd877167e3290f42c67a624ea375a46c24","modified":1556872959264},{"_id":"public/js/codeblock-resizer.js","hash":"5d0b786d60bf225d9eabcc9cece2719ff4d9b6cd","modified":1556872959264},{"_id":"public/js/search.js","hash":"0c0630e2ef213701d393b041f10572e951a27985","modified":1556872959264},{"_id":"public/donate/index.html","hash":"1845c6744cc8359df77beacff8d5bd9ce4425807","modified":1556872959264},{"_id":"public/css/default.css","hash":"7fbb18b73b44ed11193739c55fce53a6f173cf68","modified":1556872959266},{"_id":"public/css/style.css","hash":"4c1e6f7ab96bc22908ba75b2d4b61c670c27a3b1","modified":1556872959285},{"_id":"public/js/gitment.browser.js","hash":"376446d9c5930576016f97dd63e5e6616c94d8d4","modified":1556872959291}],"Category":[{"name":"SpringCloud","_id":"cjv7tymt50003c0udmr4w56c8"},{"name":"DB","_id":"cjv7tynd5000ac0udh7vqpsb9"}],"Data":[],"Page":[{"title":"关于","date":"2019-05-24T05:45:13.000Z","type":"about","comments":0,"_content":"\nEmail: [lishengming107@qq.com](lishengming107@qq.com)\nGitHub: [@li-shengming](https://github.com/li-shengming)\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2019-05-24 13:45:13\ntype: \"about\"\ncomments: false\n---\n\nEmail: [lishengming107@qq.com](lishengming107@qq.com)\nGitHub: [@li-shengming](https://github.com/li-shengming)\n","updated":"2019-05-03T01:54:40.439Z","path":"about/index.html","layout":"page","_id":"cjv7tymt20001c0udqh1lvmvo","content":"<p>Email: <a href=\"lishengming107@qq.com\">lishengming107@qq.com</a><br>GitHub: <a href=\"https://github.com/li-shengming\">@li-shengming</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Email: <a href=\"lishengming107@qq.com\">lishengming107@qq.com</a><br>GitHub: <a href=\"https://github.com/li-shengming\">@li-shengming</a></p>\n"}],"Post":[{"title":"Hello World","date":"2019-04-30T16:00:00.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2019-05-01\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"updated":"2019-05-03T07:38:43.553Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjv7tymsz0000c0uda75v5t69","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"Eureka Study","layout":"post","date":"2019-05-01T16:00:00.000Z","toc":true,"_content":"Eureka是基于REST（Representational State Transfer）服务，主要以AWS云服务为支撑，提供服务发现并实现负载均衡和故障转移。我们称此服务为Eureka服务。Eureka提供了Java客户端组件，Eureka Client，方便与服务端的交互。客户端内置了基于round-robin实现的简单负载均衡。在Netflix，为Eureka提供更为复杂的负载均衡方案进行封装，以实现高可用，它包括基于流量、资源利用率以及请求返回状态的加权负载均衡。\n\n<!-- more-->\n### Eureka介绍\nEureka包含了服务器端和客户端组件。\n- 服务器端，也被称作是服务注册中心，用于提供服务的注册与发现。Eureka支持高可用的配置，当集群中有分片出现故障时，Eureka就会转入自动保护模式，它允许分片故障期间继续提供服务的发现和注册，当故障分片恢复正常时，集群中其他分片会把他们的状态再次同步回来。\n- 客户端组件包含服务消费者与服务生产者。在应用程序运行时，Eureka客户端并把他们缓存到本地并周期性的刷新服务状态。\n\n### Eureka原理\n![Eureka原理](https://li-shengming.github.io/pictures/eureka/eureka-operate.png)\n- 第一步，spider会启动一个新的docker容器，并在新容器上部署被启动服务的镜像，然后启动服务；\n    - 如果采用的是SpringCloud框架，新起的服务会马上把信息注册到EUREKA上；\n    - 如果是非SpringCloud，周期性（默认30s，可通过eureka.client.instance-info-replication-interval-seconds调整）的将信息注册到EUREKA上；\n- 第二步，EUREKA收到客户端上报的注册信息后，将新注册服务信息放到readWriteCacheMap中，同时周期性（默认30s，可通过eureka.server.response-cache-update-interval-ms调整）的将信息刷新到缓存中（readOnlyCacheMap）中\n    - 客户端拉取信息是从缓存（readOnlyCacheMap）中拉取的【readOnlyCacheMap这个可通过eureka.server.use-read-only-response-cache=false关闭的，由于目前EUREKA是公用的，基础研发部反馈，默认配置是不能动的】；\n    - 如果你通过web页面去查看eureka上客户端的注册信息，看到的数据是readWriteCacheMap中的，也就是说，你即便在页面上看到了新启动服务的信息，也不代表调用方已经获取到了最新的实例信息；\n- 第三步，客户端会定期（默认30s,可通过eureka.client.registry-fetch-interval-seconds调整）从EUREKA上拉取最新的注册信息\n- 第四步，如果客户端采用了ribbon进行负载均衡，ribbon使用ribbon缓存进行负载均衡，客户端会定期（默认30s,可通过ribbon.ServerListRefreshInterval进行调整）最新拉取到的信息同步到ribbon缓存\n\n理解了以上四步，我们思考一个问题，新服务上线，客户端最大可能多久可以拿到最新的服务信息\n- SpringCloud下=0(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=90\n- 非SpringCloud下=30(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=120\n\n结论：90s的延迟，完全可能存在这样的场景：新服务已经起来了，老服务已经关闭，但是客户端由于没有拿到最新的地址信息，导致服务出现中断问题。\n\n上面只分析了服务上线的情况，还有服务下线的情况，如果没有做特殊配置的话，EUREKA连续3个心跳周期没有检测到客户端心跳的话，会将这一节点剔除，客户端获取到服务不可用的信息会更晚。\n\n网上查到的资料提醒：现在eureka自动下线存在BUG,自动剔除时间会翻倍，需要6个周期，最长要270s\n\n","source":"_posts/eureka-study.md","raw":"---\ntitle: Eureka Study\nlayout: post\ndate: 2019-05-02\ncategories: \n- SpringCloud\ntags: \n- Eureka\n- SourceCode\ntoc: true\n---\nEureka是基于REST（Representational State Transfer）服务，主要以AWS云服务为支撑，提供服务发现并实现负载均衡和故障转移。我们称此服务为Eureka服务。Eureka提供了Java客户端组件，Eureka Client，方便与服务端的交互。客户端内置了基于round-robin实现的简单负载均衡。在Netflix，为Eureka提供更为复杂的负载均衡方案进行封装，以实现高可用，它包括基于流量、资源利用率以及请求返回状态的加权负载均衡。\n\n<!-- more-->\n### Eureka介绍\nEureka包含了服务器端和客户端组件。\n- 服务器端，也被称作是服务注册中心，用于提供服务的注册与发现。Eureka支持高可用的配置，当集群中有分片出现故障时，Eureka就会转入自动保护模式，它允许分片故障期间继续提供服务的发现和注册，当故障分片恢复正常时，集群中其他分片会把他们的状态再次同步回来。\n- 客户端组件包含服务消费者与服务生产者。在应用程序运行时，Eureka客户端并把他们缓存到本地并周期性的刷新服务状态。\n\n### Eureka原理\n![Eureka原理](https://li-shengming.github.io/pictures/eureka/eureka-operate.png)\n- 第一步，spider会启动一个新的docker容器，并在新容器上部署被启动服务的镜像，然后启动服务；\n    - 如果采用的是SpringCloud框架，新起的服务会马上把信息注册到EUREKA上；\n    - 如果是非SpringCloud，周期性（默认30s，可通过eureka.client.instance-info-replication-interval-seconds调整）的将信息注册到EUREKA上；\n- 第二步，EUREKA收到客户端上报的注册信息后，将新注册服务信息放到readWriteCacheMap中，同时周期性（默认30s，可通过eureka.server.response-cache-update-interval-ms调整）的将信息刷新到缓存中（readOnlyCacheMap）中\n    - 客户端拉取信息是从缓存（readOnlyCacheMap）中拉取的【readOnlyCacheMap这个可通过eureka.server.use-read-only-response-cache=false关闭的，由于目前EUREKA是公用的，基础研发部反馈，默认配置是不能动的】；\n    - 如果你通过web页面去查看eureka上客户端的注册信息，看到的数据是readWriteCacheMap中的，也就是说，你即便在页面上看到了新启动服务的信息，也不代表调用方已经获取到了最新的实例信息；\n- 第三步，客户端会定期（默认30s,可通过eureka.client.registry-fetch-interval-seconds调整）从EUREKA上拉取最新的注册信息\n- 第四步，如果客户端采用了ribbon进行负载均衡，ribbon使用ribbon缓存进行负载均衡，客户端会定期（默认30s,可通过ribbon.ServerListRefreshInterval进行调整）最新拉取到的信息同步到ribbon缓存\n\n理解了以上四步，我们思考一个问题，新服务上线，客户端最大可能多久可以拿到最新的服务信息\n- SpringCloud下=0(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=90\n- 非SpringCloud下=30(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=120\n\n结论：90s的延迟，完全可能存在这样的场景：新服务已经起来了，老服务已经关闭，但是客户端由于没有拿到最新的地址信息，导致服务出现中断问题。\n\n上面只分析了服务上线的情况，还有服务下线的情况，如果没有做特殊配置的话，EUREKA连续3个心跳周期没有检测到客户端心跳的话，会将这一节点剔除，客户端获取到服务不可用的信息会更晚。\n\n网上查到的资料提醒：现在eureka自动下线存在BUG,自动剔除时间会翻倍，需要6个周期，最长要270s\n\n","slug":"eureka-study","published":1,"updated":"2019-05-03T07:37:40.732Z","comments":1,"photos":[],"link":"","_id":"cjv7tymt30002c0ud7wntg3ve","content":"<p>Eureka是基于REST（Representational State Transfer）服务，主要以AWS云服务为支撑，提供服务发现并实现负载均衡和故障转移。我们称此服务为Eureka服务。Eureka提供了Java客户端组件，Eureka Client，方便与服务端的交互。客户端内置了基于round-robin实现的简单负载均衡。在Netflix，为Eureka提供更为复杂的负载均衡方案进行封装，以实现高可用，它包括基于流量、资源利用率以及请求返回状态的加权负载均衡。</p>\n<a id=\"more\"></a>\n<h3 id=\"Eureka介绍\"><a href=\"#Eureka介绍\" class=\"headerlink\" title=\"Eureka介绍\"></a>Eureka介绍</h3><p>Eureka包含了服务器端和客户端组件。</p>\n<ul>\n<li>服务器端，也被称作是服务注册中心，用于提供服务的注册与发现。Eureka支持高可用的配置，当集群中有分片出现故障时，Eureka就会转入自动保护模式，它允许分片故障期间继续提供服务的发现和注册，当故障分片恢复正常时，集群中其他分片会把他们的状态再次同步回来。</li>\n<li>客户端组件包含服务消费者与服务生产者。在应用程序运行时，Eureka客户端并把他们缓存到本地并周期性的刷新服务状态。</li>\n</ul>\n<h3 id=\"Eureka原理\"><a href=\"#Eureka原理\" class=\"headerlink\" title=\"Eureka原理\"></a>Eureka原理</h3><p><img src=\"https://li-shengming.github.io/pictures/eureka/eureka-operate.png\" alt=\"Eureka原理\"></p>\n<ul>\n<li>第一步，spider会启动一个新的docker容器，并在新容器上部署被启动服务的镜像，然后启动服务；<ul>\n<li>如果采用的是SpringCloud框架，新起的服务会马上把信息注册到EUREKA上；</li>\n<li>如果是非SpringCloud，周期性（默认30s，可通过eureka.client.instance-info-replication-interval-seconds调整）的将信息注册到EUREKA上；</li>\n</ul>\n</li>\n<li>第二步，EUREKA收到客户端上报的注册信息后，将新注册服务信息放到readWriteCacheMap中，同时周期性（默认30s，可通过eureka.server.response-cache-update-interval-ms调整）的将信息刷新到缓存中（readOnlyCacheMap）中<ul>\n<li>客户端拉取信息是从缓存（readOnlyCacheMap）中拉取的【readOnlyCacheMap这个可通过eureka.server.use-read-only-response-cache=false关闭的，由于目前EUREKA是公用的，基础研发部反馈，默认配置是不能动的】；</li>\n<li>如果你通过web页面去查看eureka上客户端的注册信息，看到的数据是readWriteCacheMap中的，也就是说，你即便在页面上看到了新启动服务的信息，也不代表调用方已经获取到了最新的实例信息；</li>\n</ul>\n</li>\n<li>第三步，客户端会定期（默认30s,可通过eureka.client.registry-fetch-interval-seconds调整）从EUREKA上拉取最新的注册信息</li>\n<li>第四步，如果客户端采用了ribbon进行负载均衡，ribbon使用ribbon缓存进行负载均衡，客户端会定期（默认30s,可通过ribbon.ServerListRefreshInterval进行调整）最新拉取到的信息同步到ribbon缓存</li>\n</ul>\n<p>理解了以上四步，我们思考一个问题，新服务上线，客户端最大可能多久可以拿到最新的服务信息</p>\n<ul>\n<li>SpringCloud下=0(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=90</li>\n<li>非SpringCloud下=30(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=120</li>\n</ul>\n<p>结论：90s的延迟，完全可能存在这样的场景：新服务已经起来了，老服务已经关闭，但是客户端由于没有拿到最新的地址信息，导致服务出现中断问题。</p>\n<p>上面只分析了服务上线的情况，还有服务下线的情况，如果没有做特殊配置的话，EUREKA连续3个心跳周期没有检测到客户端心跳的话，会将这一节点剔除，客户端获取到服务不可用的信息会更晚。</p>\n<p>网上查到的资料提醒：现在eureka自动下线存在BUG,自动剔除时间会翻倍，需要6个周期，最长要270s</p>\n","site":{"data":{}},"excerpt":"<p>Eureka是基于REST（Representational State Transfer）服务，主要以AWS云服务为支撑，提供服务发现并实现负载均衡和故障转移。我们称此服务为Eureka服务。Eureka提供了Java客户端组件，Eureka Client，方便与服务端的交互。客户端内置了基于round-robin实现的简单负载均衡。在Netflix，为Eureka提供更为复杂的负载均衡方案进行封装，以实现高可用，它包括基于流量、资源利用率以及请求返回状态的加权负载均衡。</p>","more":"<h3 id=\"Eureka介绍\"><a href=\"#Eureka介绍\" class=\"headerlink\" title=\"Eureka介绍\"></a>Eureka介绍</h3><p>Eureka包含了服务器端和客户端组件。</p>\n<ul>\n<li>服务器端，也被称作是服务注册中心，用于提供服务的注册与发现。Eureka支持高可用的配置，当集群中有分片出现故障时，Eureka就会转入自动保护模式，它允许分片故障期间继续提供服务的发现和注册，当故障分片恢复正常时，集群中其他分片会把他们的状态再次同步回来。</li>\n<li>客户端组件包含服务消费者与服务生产者。在应用程序运行时，Eureka客户端并把他们缓存到本地并周期性的刷新服务状态。</li>\n</ul>\n<h3 id=\"Eureka原理\"><a href=\"#Eureka原理\" class=\"headerlink\" title=\"Eureka原理\"></a>Eureka原理</h3><p><img src=\"https://li-shengming.github.io/pictures/eureka/eureka-operate.png\" alt=\"Eureka原理\"></p>\n<ul>\n<li>第一步，spider会启动一个新的docker容器，并在新容器上部署被启动服务的镜像，然后启动服务；<ul>\n<li>如果采用的是SpringCloud框架，新起的服务会马上把信息注册到EUREKA上；</li>\n<li>如果是非SpringCloud，周期性（默认30s，可通过eureka.client.instance-info-replication-interval-seconds调整）的将信息注册到EUREKA上；</li>\n</ul>\n</li>\n<li>第二步，EUREKA收到客户端上报的注册信息后，将新注册服务信息放到readWriteCacheMap中，同时周期性（默认30s，可通过eureka.server.response-cache-update-interval-ms调整）的将信息刷新到缓存中（readOnlyCacheMap）中<ul>\n<li>客户端拉取信息是从缓存（readOnlyCacheMap）中拉取的【readOnlyCacheMap这个可通过eureka.server.use-read-only-response-cache=false关闭的，由于目前EUREKA是公用的，基础研发部反馈，默认配置是不能动的】；</li>\n<li>如果你通过web页面去查看eureka上客户端的注册信息，看到的数据是readWriteCacheMap中的，也就是说，你即便在页面上看到了新启动服务的信息，也不代表调用方已经获取到了最新的实例信息；</li>\n</ul>\n</li>\n<li>第三步，客户端会定期（默认30s,可通过eureka.client.registry-fetch-interval-seconds调整）从EUREKA上拉取最新的注册信息</li>\n<li>第四步，如果客户端采用了ribbon进行负载均衡，ribbon使用ribbon缓存进行负载均衡，客户端会定期（默认30s,可通过ribbon.ServerListRefreshInterval进行调整）最新拉取到的信息同步到ribbon缓存</li>\n</ul>\n<p>理解了以上四步，我们思考一个问题，新服务上线，客户端最大可能多久可以拿到最新的服务信息</p>\n<ul>\n<li>SpringCloud下=0(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=90</li>\n<li>非SpringCloud下=30(首次注册 init registe) + 30(readOnlyCacheMap)+30(client fetch interval)+30(ribbon)=120</li>\n</ul>\n<p>结论：90s的延迟，完全可能存在这样的场景：新服务已经起来了，老服务已经关闭，但是客户端由于没有拿到最新的地址信息，导致服务出现中断问题。</p>\n<p>上面只分析了服务上线的情况，还有服务下线的情况，如果没有做特殊配置的话，EUREKA连续3个心跳周期没有检测到客户端心跳的话，会将这一节点剔除，客户端获取到服务不可用的信息会更晚。</p>\n<p>网上查到的资料提醒：现在eureka自动下线存在BUG,自动剔除时间会翻倍，需要6个周期，最长要270s</p>"},{"title":"Redis Study","layout":"post","date":"2019-05-02T16:00:00.000Z","toc":true,"comments":1,"_content":" Redis是当前比较热门的NOSQL系统之一，它是一个开源的使用ANSI c语言编写的key-value存储系统（区别于MySQL的二维表格的形式存储。）。和Memcache类似，但很大程度补偿了Memcache的不足。和Memcache一样，Redis数据都是缓存在计算机内存中，不同的是，Memcache只能将数据缓存到内存中，无法自动定期写入硬盘，这就表示，一断电或重启，内存清空，数据丢失。所以Memcache的应用场景适用于缓存无需持久化的数据。而Redis不同的是它会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，实现数据的持久化.\n<!--more-->\n## Redis的机制\n### 高性能\nRedis最牛逼的地方就是快，为什么这么快呢，主要原因如下\n- 纯内存操作\n- 数据结构简单，对数据操作也简单\n- 单线程操作，避免了频繁的上下文切换\n    单线程好处：1.代码更清晰，处理逻辑更简单；2.不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；3.不存在多进程或者多线程导致的切换而消耗CPU\n    单线程劣势：无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善；\n    \n- 采用了[非阻塞I/O多路复用机制](https://blog.csdn.net/happy_wu/article/details/80052617)\n    IO多路复用是需要操作系统支持的，目前，提供的多路复用函数库主要有四下个方法[select、poll、epoll、kqueue等](https://www.cnblogs.com/skiler/p/6852493.html)\n    名词比较绕口，理解涵义就好。从网上抄来的一个epoll场景：\n    一个酒吧服务员（一个线程），前面趴了一群醉汉，突然一个吼一声“倒酒”（事件），你小跑过去给他倒一杯，然后随他去吧，突然又一个要倒酒，你又过去倒上，就这样一个服务员服务好多人，有时没人喝酒，服务员处于空闲状态，可以干点别的玩玩手机。至于epoll与select，poll的区别在于后两者的场景中醉汉不说话，你要挨个问要不要酒，没时间玩手机了。epoll是linux下内核支持的函数，而kqueue是其他系统支持的函数。io多路复用大概就是指这几个醉汉共用一个服务员。\n\n### 高可用性\n###### 了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\nRedis还有一个很牛逼的特性就是高可用性，Redis是如何做到的呢，主要原因如下\n- 支持持久化机制，用于解决机器故障导致内存数据丢失的问题\n- 支持主从复制，用于解决读取数据压力大问题\n- 支持哨兵机制，能够及时做到故障转移\n- 支持集群机制，能够扩充Redis可以存放的数据量\n- 支持过期策略以及内存淘汰机制，解决写入数据高于Redis总内存问题\n- 事务控制和分布式锁，保证并发场景下正常的使用Redis\n\n下面依次分析这几个机制\n### 持久化机制\n#### 持久化作用\nRedis持久化就是按照一定的机制及时将内存里的数据同步至磁盘。\n#### 为什么要去支持持久化呢？\nRedis之前比较流行的内存数据库是memcached，由于它的数据都存储到内存里了，就导致它有一个很致命的缺陷，就是一旦机制出现故障，所有数据就会丢失，这一点就一定决定了memcached的应用场景会非常受限，只能应用于一些非核心业务，可以接受数据丢失。\n#### 持久化机制\n- RDB\n可以在指定的时间间隔内生成数据集的时间点快照\n- AOF\n持久化记录服务器执行的所有写操作命令\n- 同时启用AOF和RDB\n 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。为了缓解这一问题，Redis支持Rewrite重写，简单的讲就是基于一定算法，给Aof文件“瘦身”。\n\n优缺点分析\n- RDB Redis会单独启动一个进程，先把数据收集到一个临时文件，等待上一个持久化操作结束后，并且按照你配置的持久策略去把数据同步到磁盘，对主进程无影响，性能高，但是缺点是，一旦出现机器故障，放在缓存里的数据就会丢失。\nRDB策略配置示例如下（redis.conf）\n每900s有一次写操作触发同步，或者每300s有10次写操作触发同步，或者每60s有10000次操作触发同步\n```\nsave 900 1\nsave 300 10\nsave 60 10000\n```\n- AOF Redis会将自己所有执行过的更新指令记录下来，并且日志文件只允许尾部添加操作。恢复数据的时候，Redis会从头到尾重新执行一遍。\nAof目前同步策略：Always（每步）Everysec（每一秒）No（操作系统决定）。\n开启aof配置，以及同步策略配置示例如下（redis.conf）\n开启aof，每秒同步一次写操作到磁盘\n```\nappendonly yes \nappendfsync everysec\n```\n这两种机制没有好坏之说，使用的时候，需要你去分析自己的实际业务场景，哪个适合用哪个！\n参考链接：[Redis持久化](https://www.cnblogs.com/shsxt/p/7911591.html)\n\n### 主从复制\n#### 作用\nRedis相对于传统的db，读写非常快的，但是，面对更大的读请求来临时，Redis提供了主从复制策略来解决读取数据压力大问题\n#### 结构\nRedis 主从master-slave级联图\n\n![redis-master-slave](https://li-shengming.github.io/pictures/redis/redis-master-slave.png)\n\n**主从同步策略**\n\n主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。\n开启从服务器配置（redis.conf）\n将当前服务器配置成192.168.1.1 6379 这个节点的从服务器\n```\nslaveof 192.168.1.1 6379\n```\n参考链接：[主从复制](https://www.cnblogs.com/hepingqingfeng/p/7263782.html)\n\n### 哨兵\n### 作用\n如果Redis主节点跪了的话，如何来保证Redis系统可用性呢？Redis提供了哨兵机制，哨兵会去监控Redis节点，当发现Redis主节点跪了的时候，会通过投票机制，从从节点中挑取一个节点自动升级为主节点，自动实现故障转移，同事还会向管理员发送Redis节点异常通知。\n#### 结构\nRedis 哨兵监控图(哨兵也可以多个)\n![redis-master-slave](https://li-shengming.github.io/pictures/redis/redis-sentinel.png)\n**哨兵原理介绍**\n\n首先理解两个名词\n- 主观下线：当前哨兵节点连接某一Redis节点失败；\n- 客观下线：当sentinel监视的某个服务主观下线后，sentinel会询问其它监视该服务的sentinel，看它们是否也认为该服务主观下线，接收到足够数量（这个值可以配置）的sentinel判断为主观下线，既任务该服务客观下线，并对其做故障转移操作。\n选举领头哨兵\n\n 一个redis服务被判断为客观下线时，多个监视该服务的sentinel协商，选举一个领头sentinel，对该redis服务进行古战转移操作。选举领头sentinel会遵循一些规则，有兴趣可以自行调研。\n    注：你可能会关心为啥选举领头哨兵，其实道理很简单，做故障迁移的时候，不可能每个哨兵节点都私自决定用哪个节点替代坏掉的主节点，\n    会乱套的，公平的投出一个节点作为领头者，然后基于一定规则去做故障转移最快\n进行故障转移，分为三个主要步骤：\n 1) 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务；\n 2) 已下线主服务的所有从服务改为复制新的主服务；\n 3) 将已下线的主服务设置成新的主服务的从服务，当其回复正常时，复制新的主服务，变成新的主服务的从服务。\n参考链接：[Redis哨兵](https://blog.csdn.net/RobertoHuang/article/details/70768922)\n### 集群\n#### 作用\n一台Redis节点内存资源总是有限的，如果不够使的话，怎么能扩展吗？\nRedis是支持集群的，默认关闭。\n开启集群配置(redis.conf)\n```\ncluster-enabled yes #开启cluster\n\n```\n#### Redis集群架构\n![redis-cluster](https://li-shengming.github.io/pictures/redis/redis-cluster.png)\n图上能看到的信息：\n1) 对象保存到Redis之前先经过CRC16哈希到一个指定的Node上，例如Object4最终Hash到了Node1上。\n2) 每个Node被平均分配了一个Slot段，对应着0-16384，Slot不能重复也不能缺失，否则会导致对象重复存储或无法存储。\n3) Node之间也互相监听，一旦有Node退出或者加入，会按照Slot为单位做数据的迁移。例如Node1如果掉线了，0-5640这些Slot将会平均分摊到Node2和Node3上,由于Node2和Node3本身维护的Slot还会在自己身上不会被重新分配，所以迁移过程中不会影响到5641-16384Slot段的使用。\n\n简单总结下哈希Slot的优缺点：\n- 优点：将Redis的写操作分摊到了多个节点上，提高写的并发能力，扩容简单。\n- 缺点：每个Node承担着互相监听、高并发数据写入、高并发数据读出，工作任务繁重\n\n延展性问题：redis为什么采用一致性hash，而没有采用传统的取模算法？\n其实道理很简答，因为在集群环境下，新增节点或者节点失效是很常见的，如果采用取模的那种算法，每次新增或者删除节点时，所有与数据都可能需要重新调整位置，这是无法接受的！\n结合redis集群和主从复制两种思想，可以得到Redis集群的最终形态\n\n![redis-cluster-final](https://li-shengming.github.io/pictures/redis/redis-cluster-final.png)\n\n想扩展并发读就添加Slaver，想扩展并发写就添加Master，想扩容也就是添加Master，任何一个Slaver或者几个Master挂了都不会是灾难性的故障。完美！\n参考链接：[Redis集群设计原理](https://blog.csdn.net/yejingtao703/article/details/78484151)\n### 内存数据管理方案\n#### 作用\n这一部分对应的问题很常见，就是如果你的redis只能存5g数据，可是你写入了10g数据，那么内存里最终会保留哪5g数据的问题。\nredis采用的是定期删除+惰性删除策略。\n\n**为什么不用定时删除策略?**\n定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.\n\n**定期删除+惰性删除是如何工作的呢?**\n定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。\n于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。\n\n**采用定期删除+惰性删除就没其他问题了么?**\n不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。\n在redis.conf中有一行配置示例(当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key)\n```\nmaxmemory-policy volatile-lru\n```\n参考链接：[分布式之redis复习精讲](https://www.cnblogs.com/rjzheng/p/9096228.html)\n\n### Redis事务和分布式锁\n#### Redis事务\n核心操作：multi开启事务，exec执行事务，discard撤销事务，watch监控key是否改过、UNWATCH对应watch操作【multi和exec执行后会自动unwatch调监控队列里的所有key，如果想提前释放，可以用这个命令】\n\nredis事务采用的是乐观锁，就是先处理，再真正去执行的时候去看有没有被修改过，没有的话，就去更新，否则报错。这个思想是不是有点似曾相识，是的，concurrenthashmap的set值时也是用的这个思想。还是那句话，虽然技术上感觉不搭边，但是思想上确实想通的。\n\n#### Redis分布式锁\n核心操作：setNX，理解这个操作基本上就理解了分布式锁。这个方法的意思就是去给一个key“赋值”，如果之前没人这样做过，返回值是1，否则返回0.\n\n\n参考链接：[Redis事务和分布式锁](https://www.cnblogs.com/Jason-Xiang/p/5364252.html)\n## Redis的缺点\nRedis会是完美的程序吗？肯定不可能的，世界上就没有完美的东西。我们需要做的事情，分析清楚事物的主要矛盾，在实际开发中，不要犯这些错误就好了。\n\n以下典型问题，重点参考了[分布式之redis复习精讲](https://www.cnblogs.com/rjzheng/p/9096228.html)\n### 缓存和数据库双写一致性问题\n#### 问题说明\n一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。\n#### 解决方案\n详细解决方案见[分布式之数据库和缓存双写一致性方案解析](https://www.cnblogs.com/rjzheng/p/9041659.html)\n\n### 缓存雪崩问题\n缓存击穿问题和缓存雪崩问题都是大项目中可能会遇到，小项目比较难遇到。\n#### 问题说明\n黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常\n#### 解决方案\n1) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试\n2) 采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。\n3) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。\n\n### 缓存击穿问题\n#### 问题说明\n缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常\n#### 解决方案\n1) 给缓存的失效时间，加上一个随机值，避免集体失效。\n2) 使用互斥锁，但是该方案吞吐量明显下降了。\n3) 双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点\n- I 从缓存A读数据库，有则直接返回\n- II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。\n- III 更新线程同时更新缓存A和缓存B。\n\n### 缓存的并发竞争问题\n#### 问题说明\n这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。\n#### 解决方案\n1) 如果对这个key操作，不要求顺序\n这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。\n2) 如果对这个key操作，要求顺序\n假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.\n期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳\n","source":"_posts/redis-study.md","raw":"---\ntitle: Redis Study\nlayout: post\ndate: 2019-05-03\ncategories: \n- DB\ntags: \n- Redis\n- SourceCode\ntoc: true\ncomments: true\n---\n Redis是当前比较热门的NOSQL系统之一，它是一个开源的使用ANSI c语言编写的key-value存储系统（区别于MySQL的二维表格的形式存储。）。和Memcache类似，但很大程度补偿了Memcache的不足。和Memcache一样，Redis数据都是缓存在计算机内存中，不同的是，Memcache只能将数据缓存到内存中，无法自动定期写入硬盘，这就表示，一断电或重启，内存清空，数据丢失。所以Memcache的应用场景适用于缓存无需持久化的数据。而Redis不同的是它会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，实现数据的持久化.\n<!--more-->\n## Redis的机制\n### 高性能\nRedis最牛逼的地方就是快，为什么这么快呢，主要原因如下\n- 纯内存操作\n- 数据结构简单，对数据操作也简单\n- 单线程操作，避免了频繁的上下文切换\n    单线程好处：1.代码更清晰，处理逻辑更简单；2.不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；3.不存在多进程或者多线程导致的切换而消耗CPU\n    单线程劣势：无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善；\n    \n- 采用了[非阻塞I/O多路复用机制](https://blog.csdn.net/happy_wu/article/details/80052617)\n    IO多路复用是需要操作系统支持的，目前，提供的多路复用函数库主要有四下个方法[select、poll、epoll、kqueue等](https://www.cnblogs.com/skiler/p/6852493.html)\n    名词比较绕口，理解涵义就好。从网上抄来的一个epoll场景：\n    一个酒吧服务员（一个线程），前面趴了一群醉汉，突然一个吼一声“倒酒”（事件），你小跑过去给他倒一杯，然后随他去吧，突然又一个要倒酒，你又过去倒上，就这样一个服务员服务好多人，有时没人喝酒，服务员处于空闲状态，可以干点别的玩玩手机。至于epoll与select，poll的区别在于后两者的场景中醉汉不说话，你要挨个问要不要酒，没时间玩手机了。epoll是linux下内核支持的函数，而kqueue是其他系统支持的函数。io多路复用大概就是指这几个醉汉共用一个服务员。\n\n### 高可用性\n###### 了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\nRedis还有一个很牛逼的特性就是高可用性，Redis是如何做到的呢，主要原因如下\n- 支持持久化机制，用于解决机器故障导致内存数据丢失的问题\n- 支持主从复制，用于解决读取数据压力大问题\n- 支持哨兵机制，能够及时做到故障转移\n- 支持集群机制，能够扩充Redis可以存放的数据量\n- 支持过期策略以及内存淘汰机制，解决写入数据高于Redis总内存问题\n- 事务控制和分布式锁，保证并发场景下正常的使用Redis\n\n下面依次分析这几个机制\n### 持久化机制\n#### 持久化作用\nRedis持久化就是按照一定的机制及时将内存里的数据同步至磁盘。\n#### 为什么要去支持持久化呢？\nRedis之前比较流行的内存数据库是memcached，由于它的数据都存储到内存里了，就导致它有一个很致命的缺陷，就是一旦机制出现故障，所有数据就会丢失，这一点就一定决定了memcached的应用场景会非常受限，只能应用于一些非核心业务，可以接受数据丢失。\n#### 持久化机制\n- RDB\n可以在指定的时间间隔内生成数据集的时间点快照\n- AOF\n持久化记录服务器执行的所有写操作命令\n- 同时启用AOF和RDB\n 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。为了缓解这一问题，Redis支持Rewrite重写，简单的讲就是基于一定算法，给Aof文件“瘦身”。\n\n优缺点分析\n- RDB Redis会单独启动一个进程，先把数据收集到一个临时文件，等待上一个持久化操作结束后，并且按照你配置的持久策略去把数据同步到磁盘，对主进程无影响，性能高，但是缺点是，一旦出现机器故障，放在缓存里的数据就会丢失。\nRDB策略配置示例如下（redis.conf）\n每900s有一次写操作触发同步，或者每300s有10次写操作触发同步，或者每60s有10000次操作触发同步\n```\nsave 900 1\nsave 300 10\nsave 60 10000\n```\n- AOF Redis会将自己所有执行过的更新指令记录下来，并且日志文件只允许尾部添加操作。恢复数据的时候，Redis会从头到尾重新执行一遍。\nAof目前同步策略：Always（每步）Everysec（每一秒）No（操作系统决定）。\n开启aof配置，以及同步策略配置示例如下（redis.conf）\n开启aof，每秒同步一次写操作到磁盘\n```\nappendonly yes \nappendfsync everysec\n```\n这两种机制没有好坏之说，使用的时候，需要你去分析自己的实际业务场景，哪个适合用哪个！\n参考链接：[Redis持久化](https://www.cnblogs.com/shsxt/p/7911591.html)\n\n### 主从复制\n#### 作用\nRedis相对于传统的db，读写非常快的，但是，面对更大的读请求来临时，Redis提供了主从复制策略来解决读取数据压力大问题\n#### 结构\nRedis 主从master-slave级联图\n\n![redis-master-slave](https://li-shengming.github.io/pictures/redis/redis-master-slave.png)\n\n**主从同步策略**\n\n主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。\n开启从服务器配置（redis.conf）\n将当前服务器配置成192.168.1.1 6379 这个节点的从服务器\n```\nslaveof 192.168.1.1 6379\n```\n参考链接：[主从复制](https://www.cnblogs.com/hepingqingfeng/p/7263782.html)\n\n### 哨兵\n### 作用\n如果Redis主节点跪了的话，如何来保证Redis系统可用性呢？Redis提供了哨兵机制，哨兵会去监控Redis节点，当发现Redis主节点跪了的时候，会通过投票机制，从从节点中挑取一个节点自动升级为主节点，自动实现故障转移，同事还会向管理员发送Redis节点异常通知。\n#### 结构\nRedis 哨兵监控图(哨兵也可以多个)\n![redis-master-slave](https://li-shengming.github.io/pictures/redis/redis-sentinel.png)\n**哨兵原理介绍**\n\n首先理解两个名词\n- 主观下线：当前哨兵节点连接某一Redis节点失败；\n- 客观下线：当sentinel监视的某个服务主观下线后，sentinel会询问其它监视该服务的sentinel，看它们是否也认为该服务主观下线，接收到足够数量（这个值可以配置）的sentinel判断为主观下线，既任务该服务客观下线，并对其做故障转移操作。\n选举领头哨兵\n\n 一个redis服务被判断为客观下线时，多个监视该服务的sentinel协商，选举一个领头sentinel，对该redis服务进行古战转移操作。选举领头sentinel会遵循一些规则，有兴趣可以自行调研。\n    注：你可能会关心为啥选举领头哨兵，其实道理很简单，做故障迁移的时候，不可能每个哨兵节点都私自决定用哪个节点替代坏掉的主节点，\n    会乱套的，公平的投出一个节点作为领头者，然后基于一定规则去做故障转移最快\n进行故障转移，分为三个主要步骤：\n 1) 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务；\n 2) 已下线主服务的所有从服务改为复制新的主服务；\n 3) 将已下线的主服务设置成新的主服务的从服务，当其回复正常时，复制新的主服务，变成新的主服务的从服务。\n参考链接：[Redis哨兵](https://blog.csdn.net/RobertoHuang/article/details/70768922)\n### 集群\n#### 作用\n一台Redis节点内存资源总是有限的，如果不够使的话，怎么能扩展吗？\nRedis是支持集群的，默认关闭。\n开启集群配置(redis.conf)\n```\ncluster-enabled yes #开启cluster\n\n```\n#### Redis集群架构\n![redis-cluster](https://li-shengming.github.io/pictures/redis/redis-cluster.png)\n图上能看到的信息：\n1) 对象保存到Redis之前先经过CRC16哈希到一个指定的Node上，例如Object4最终Hash到了Node1上。\n2) 每个Node被平均分配了一个Slot段，对应着0-16384，Slot不能重复也不能缺失，否则会导致对象重复存储或无法存储。\n3) Node之间也互相监听，一旦有Node退出或者加入，会按照Slot为单位做数据的迁移。例如Node1如果掉线了，0-5640这些Slot将会平均分摊到Node2和Node3上,由于Node2和Node3本身维护的Slot还会在自己身上不会被重新分配，所以迁移过程中不会影响到5641-16384Slot段的使用。\n\n简单总结下哈希Slot的优缺点：\n- 优点：将Redis的写操作分摊到了多个节点上，提高写的并发能力，扩容简单。\n- 缺点：每个Node承担着互相监听、高并发数据写入、高并发数据读出，工作任务繁重\n\n延展性问题：redis为什么采用一致性hash，而没有采用传统的取模算法？\n其实道理很简答，因为在集群环境下，新增节点或者节点失效是很常见的，如果采用取模的那种算法，每次新增或者删除节点时，所有与数据都可能需要重新调整位置，这是无法接受的！\n结合redis集群和主从复制两种思想，可以得到Redis集群的最终形态\n\n![redis-cluster-final](https://li-shengming.github.io/pictures/redis/redis-cluster-final.png)\n\n想扩展并发读就添加Slaver，想扩展并发写就添加Master，想扩容也就是添加Master，任何一个Slaver或者几个Master挂了都不会是灾难性的故障。完美！\n参考链接：[Redis集群设计原理](https://blog.csdn.net/yejingtao703/article/details/78484151)\n### 内存数据管理方案\n#### 作用\n这一部分对应的问题很常见，就是如果你的redis只能存5g数据，可是你写入了10g数据，那么内存里最终会保留哪5g数据的问题。\nredis采用的是定期删除+惰性删除策略。\n\n**为什么不用定时删除策略?**\n定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.\n\n**定期删除+惰性删除是如何工作的呢?**\n定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。\n于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。\n\n**采用定期删除+惰性删除就没其他问题了么?**\n不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。\n在redis.conf中有一行配置示例(当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key)\n```\nmaxmemory-policy volatile-lru\n```\n参考链接：[分布式之redis复习精讲](https://www.cnblogs.com/rjzheng/p/9096228.html)\n\n### Redis事务和分布式锁\n#### Redis事务\n核心操作：multi开启事务，exec执行事务，discard撤销事务，watch监控key是否改过、UNWATCH对应watch操作【multi和exec执行后会自动unwatch调监控队列里的所有key，如果想提前释放，可以用这个命令】\n\nredis事务采用的是乐观锁，就是先处理，再真正去执行的时候去看有没有被修改过，没有的话，就去更新，否则报错。这个思想是不是有点似曾相识，是的，concurrenthashmap的set值时也是用的这个思想。还是那句话，虽然技术上感觉不搭边，但是思想上确实想通的。\n\n#### Redis分布式锁\n核心操作：setNX，理解这个操作基本上就理解了分布式锁。这个方法的意思就是去给一个key“赋值”，如果之前没人这样做过，返回值是1，否则返回0.\n\n\n参考链接：[Redis事务和分布式锁](https://www.cnblogs.com/Jason-Xiang/p/5364252.html)\n## Redis的缺点\nRedis会是完美的程序吗？肯定不可能的，世界上就没有完美的东西。我们需要做的事情，分析清楚事物的主要矛盾，在实际开发中，不要犯这些错误就好了。\n\n以下典型问题，重点参考了[分布式之redis复习精讲](https://www.cnblogs.com/rjzheng/p/9096228.html)\n### 缓存和数据库双写一致性问题\n#### 问题说明\n一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。\n#### 解决方案\n详细解决方案见[分布式之数据库和缓存双写一致性方案解析](https://www.cnblogs.com/rjzheng/p/9041659.html)\n\n### 缓存雪崩问题\n缓存击穿问题和缓存雪崩问题都是大项目中可能会遇到，小项目比较难遇到。\n#### 问题说明\n黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常\n#### 解决方案\n1) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试\n2) 采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。\n3) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。\n\n### 缓存击穿问题\n#### 问题说明\n缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常\n#### 解决方案\n1) 给缓存的失效时间，加上一个随机值，避免集体失效。\n2) 使用互斥锁，但是该方案吞吐量明显下降了。\n3) 双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点\n- I 从缓存A读数据库，有则直接返回\n- II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。\n- III 更新线程同时更新缓存A和缓存B。\n\n### 缓存的并发竞争问题\n#### 问题说明\n这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。\n#### 解决方案\n1) 如果对这个key操作，不要求顺序\n这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。\n2) 如果对这个key操作，要求顺序\n假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.\n期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳\n","slug":"redis-study","published":1,"updated":"2019-05-03T08:18:07.651Z","photos":[],"link":"","_id":"cjv7tynd40009c0udcn22i0zm","content":"<p> Redis是当前比较热门的NOSQL系统之一，它是一个开源的使用ANSI c语言编写的key-value存储系统（区别于MySQL的二维表格的形式存储。）。和Memcache类似，但很大程度补偿了Memcache的不足。和Memcache一样，Redis数据都是缓存在计算机内存中，不同的是，Memcache只能将数据缓存到内存中，无法自动定期写入硬盘，这就表示，一断电或重启，内存清空，数据丢失。所以Memcache的应用场景适用于缓存无需持久化的数据。而Redis不同的是它会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，实现数据的持久化.<br><a id=\"more\"></a></p>\n<h2 id=\"Redis的机制\"><a href=\"#Redis的机制\" class=\"headerlink\" title=\"Redis的机制\"></a>Redis的机制</h2><h3 id=\"高性能\"><a href=\"#高性能\" class=\"headerlink\" title=\"高性能\"></a>高性能</h3><p>Redis最牛逼的地方就是快，为什么这么快呢，主要原因如下</p>\n<ul>\n<li>纯内存操作</li>\n<li>数据结构简单，对数据操作也简单</li>\n<li><p>单线程操作，避免了频繁的上下文切换<br>  单线程好处：1.代码更清晰，处理逻辑更简单；2.不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；3.不存在多进程或者多线程导致的切换而消耗CPU<br>  单线程劣势：无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善；</p>\n</li>\n<li><p>采用了<a href=\"https://blog.csdn.net/happy_wu/article/details/80052617\" target=\"_blank\" rel=\"noopener\">非阻塞I/O多路复用机制</a><br>  IO多路复用是需要操作系统支持的，目前，提供的多路复用函数库主要有四下个方法<a href=\"https://www.cnblogs.com/skiler/p/6852493.html\" target=\"_blank\" rel=\"noopener\">select、poll、epoll、kqueue等</a><br>  名词比较绕口，理解涵义就好。从网上抄来的一个epoll场景：<br>  一个酒吧服务员（一个线程），前面趴了一群醉汉，突然一个吼一声“倒酒”（事件），你小跑过去给他倒一杯，然后随他去吧，突然又一个要倒酒，你又过去倒上，就这样一个服务员服务好多人，有时没人喝酒，服务员处于空闲状态，可以干点别的玩玩手机。至于epoll与select，poll的区别在于后两者的场景中醉汉不说话，你要挨个问要不要酒，没时间玩手机了。epoll是linux下内核支持的函数，而kqueue是其他系统支持的函数。io多路复用大概就是指这几个醉汉共用一个服务员。</p>\n</li>\n</ul>\n<h3 id=\"高可用性\"><a href=\"#高可用性\" class=\"headerlink\" title=\"高可用性\"></a>高可用性</h3><h6 id=\"了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\"><a href=\"#了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\" class=\"headerlink\" title=\"了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\"></a>了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。</h6><p>Redis还有一个很牛逼的特性就是高可用性，Redis是如何做到的呢，主要原因如下</p>\n<ul>\n<li>支持持久化机制，用于解决机器故障导致内存数据丢失的问题</li>\n<li>支持主从复制，用于解决读取数据压力大问题</li>\n<li>支持哨兵机制，能够及时做到故障转移</li>\n<li>支持集群机制，能够扩充Redis可以存放的数据量</li>\n<li>支持过期策略以及内存淘汰机制，解决写入数据高于Redis总内存问题</li>\n<li>事务控制和分布式锁，保证并发场景下正常的使用Redis</li>\n</ul>\n<p>下面依次分析这几个机制</p>\n<h3 id=\"持久化机制\"><a href=\"#持久化机制\" class=\"headerlink\" title=\"持久化机制\"></a>持久化机制</h3><h4 id=\"持久化作用\"><a href=\"#持久化作用\" class=\"headerlink\" title=\"持久化作用\"></a>持久化作用</h4><p>Redis持久化就是按照一定的机制及时将内存里的数据同步至磁盘。</p>\n<h4 id=\"为什么要去支持持久化呢？\"><a href=\"#为什么要去支持持久化呢？\" class=\"headerlink\" title=\"为什么要去支持持久化呢？\"></a>为什么要去支持持久化呢？</h4><p>Redis之前比较流行的内存数据库是memcached，由于它的数据都存储到内存里了，就导致它有一个很致命的缺陷，就是一旦机制出现故障，所有数据就会丢失，这一点就一定决定了memcached的应用场景会非常受限，只能应用于一些非核心业务，可以接受数据丢失。</p>\n<h4 id=\"持久化机制-1\"><a href=\"#持久化机制-1\" class=\"headerlink\" title=\"持久化机制\"></a>持久化机制</h4><ul>\n<li>RDB<br>可以在指定的时间间隔内生成数据集的时间点快照</li>\n<li>AOF<br>持久化记录服务器执行的所有写操作命令</li>\n<li>同时启用AOF和RDB<br>当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。为了缓解这一问题，Redis支持Rewrite重写，简单的讲就是基于一定算法，给Aof文件“瘦身”。</li>\n</ul>\n<p>优缺点分析</p>\n<ul>\n<li><p>RDB Redis会单独启动一个进程，先把数据收集到一个临时文件，等待上一个持久化操作结束后，并且按照你配置的持久策略去把数据同步到磁盘，对主进程无影响，性能高，但是缺点是，一旦出现机器故障，放在缓存里的数据就会丢失。<br>RDB策略配置示例如下（redis.conf）<br>每900s有一次写操作触发同步，或者每300s有10次写操作触发同步，或者每60s有10000次操作触发同步</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">save 900 1</span><br><span class=\"line\">save 300 10</span><br><span class=\"line\">save 60 10000</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>AOF Redis会将自己所有执行过的更新指令记录下来，并且日志文件只允许尾部添加操作。恢复数据的时候，Redis会从头到尾重新执行一遍。<br>Aof目前同步策略：Always（每步）Everysec（每一秒）No（操作系统决定）。<br>开启aof配置，以及同步策略配置示例如下（redis.conf）<br>开启aof，每秒同步一次写操作到磁盘</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">appendonly yes </span><br><span class=\"line\">appendfsync everysec</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>这两种机制没有好坏之说，使用的时候，需要你去分析自己的实际业务场景，哪个适合用哪个！<br>参考链接：<a href=\"https://www.cnblogs.com/shsxt/p/7911591.html\" target=\"_blank\" rel=\"noopener\">Redis持久化</a></p>\n<h3 id=\"主从复制\"><a href=\"#主从复制\" class=\"headerlink\" title=\"主从复制\"></a>主从复制</h3><h4 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>Redis相对于传统的db，读写非常快的，但是，面对更大的读请求来临时，Redis提供了主从复制策略来解决读取数据压力大问题</p>\n<h4 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h4><p>Redis 主从master-slave级联图</p>\n<p><img src=\"https://li-shengming.github.io/pictures/redis/redis-master-slave.png\" alt=\"redis-master-slave\"></p>\n<p><strong>主从同步策略</strong></p>\n<p>主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。<br>开启从服务器配置（redis.conf）<br>将当前服务器配置成192.168.1.1 6379 这个节点的从服务器<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">slaveof 192.168.1.1 6379</span><br></pre></td></tr></table></figure></p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/hepingqingfeng/p/7263782.html\" target=\"_blank\" rel=\"noopener\">主从复制</a></p>\n<h3 id=\"哨兵\"><a href=\"#哨兵\" class=\"headerlink\" title=\"哨兵\"></a>哨兵</h3><h3 id=\"作用-1\"><a href=\"#作用-1\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>如果Redis主节点跪了的话，如何来保证Redis系统可用性呢？Redis提供了哨兵机制，哨兵会去监控Redis节点，当发现Redis主节点跪了的时候，会通过投票机制，从从节点中挑取一个节点自动升级为主节点，自动实现故障转移，同事还会向管理员发送Redis节点异常通知。</p>\n<h4 id=\"结构-1\"><a href=\"#结构-1\" class=\"headerlink\" title=\"结构\"></a>结构</h4><p>Redis 哨兵监控图(哨兵也可以多个)<br><img src=\"https://li-shengming.github.io/pictures/redis/redis-sentinel.png\" alt=\"redis-master-slave\"><br><strong>哨兵原理介绍</strong></p>\n<p>首先理解两个名词</p>\n<ul>\n<li>主观下线：当前哨兵节点连接某一Redis节点失败；</li>\n<li><p>客观下线：当sentinel监视的某个服务主观下线后，sentinel会询问其它监视该服务的sentinel，看它们是否也认为该服务主观下线，接收到足够数量（这个值可以配置）的sentinel判断为主观下线，既任务该服务客观下线，并对其做故障转移操作。<br>选举领头哨兵</p>\n<p>一个redis服务被判断为客观下线时，多个监视该服务的sentinel协商，选举一个领头sentinel，对该redis服务进行古战转移操作。选举领头sentinel会遵循一些规则，有兴趣可以自行调研。<br>  注：你可能会关心为啥选举领头哨兵，其实道理很简单，做故障迁移的时候，不可能每个哨兵节点都私自决定用哪个节点替代坏掉的主节点，<br>  会乱套的，公平的投出一个节点作为领头者，然后基于一定规则去做故障转移最快<br>进行故障转移，分为三个主要步骤：<br>1) 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务；<br>2) 已下线主服务的所有从服务改为复制新的主服务；<br>3) 将已下线的主服务设置成新的主服务的从服务，当其回复正常时，复制新的主服务，变成新的主服务的从服务。<br>参考链接：<a href=\"https://blog.csdn.net/RobertoHuang/article/details/70768922\" target=\"_blank\" rel=\"noopener\">Redis哨兵</a></p>\n<h3 id=\"集群\"><a href=\"#集群\" class=\"headerlink\" title=\"集群\"></a>集群</h3><h4 id=\"作用-2\"><a href=\"#作用-2\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>一台Redis节点内存资源总是有限的，如果不够使的话，怎么能扩展吗？<br>Redis是支持集群的，默认关闭。<br>开启集群配置(redis.conf)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster-enabled yes #开启cluster</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Redis集群架构\"><a href=\"#Redis集群架构\" class=\"headerlink\" title=\"Redis集群架构\"></a>Redis集群架构</h4><p><img src=\"https://li-shengming.github.io/pictures/redis/redis-cluster.png\" alt=\"redis-cluster\"><br>图上能看到的信息：<br>1) 对象保存到Redis之前先经过CRC16哈希到一个指定的Node上，例如Object4最终Hash到了Node1上。<br>2) 每个Node被平均分配了一个Slot段，对应着0-16384，Slot不能重复也不能缺失，否则会导致对象重复存储或无法存储。<br>3) Node之间也互相监听，一旦有Node退出或者加入，会按照Slot为单位做数据的迁移。例如Node1如果掉线了，0-5640这些Slot将会平均分摊到Node2和Node3上,由于Node2和Node3本身维护的Slot还会在自己身上不会被重新分配，所以迁移过程中不会影响到5641-16384Slot段的使用。</p>\n<p>简单总结下哈希Slot的优缺点：</p>\n<ul>\n<li>优点：将Redis的写操作分摊到了多个节点上，提高写的并发能力，扩容简单。</li>\n<li>缺点：每个Node承担着互相监听、高并发数据写入、高并发数据读出，工作任务繁重</li>\n</ul>\n<p>延展性问题：redis为什么采用一致性hash，而没有采用传统的取模算法？<br>其实道理很简答，因为在集群环境下，新增节点或者节点失效是很常见的，如果采用取模的那种算法，每次新增或者删除节点时，所有与数据都可能需要重新调整位置，这是无法接受的！<br>结合redis集群和主从复制两种思想，可以得到Redis集群的最终形态</p>\n<p><img src=\"https://li-shengming.github.io/pictures/redis/redis-cluster-final.png\" alt=\"redis-cluster-final\"></p>\n<p>想扩展并发读就添加Slaver，想扩展并发写就添加Master，想扩容也就是添加Master，任何一个Slaver或者几个Master挂了都不会是灾难性的故障。完美！<br>参考链接：<a href=\"https://blog.csdn.net/yejingtao703/article/details/78484151\" target=\"_blank\" rel=\"noopener\">Redis集群设计原理</a></p>\n<h3 id=\"内存数据管理方案\"><a href=\"#内存数据管理方案\" class=\"headerlink\" title=\"内存数据管理方案\"></a>内存数据管理方案</h3><h4 id=\"作用-3\"><a href=\"#作用-3\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>这一部分对应的问题很常见，就是如果你的redis只能存5g数据，可是你写入了10g数据，那么内存里最终会保留哪5g数据的问题。<br>redis采用的是定期删除+惰性删除策略。</p>\n<p><strong>为什么不用定时删除策略?</strong><br>定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.</p>\n<p><strong>定期删除+惰性删除是如何工作的呢?</strong><br>定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。<br>于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。</p>\n<p><strong>采用定期删除+惰性删除就没其他问题了么?</strong><br>不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。<br>在redis.conf中有一行配置示例(当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">maxmemory-policy volatile-lru</span><br></pre></td></tr></table></figure></p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/rjzheng/p/9096228.html\" target=\"_blank\" rel=\"noopener\">分布式之redis复习精讲</a></p>\n<h3 id=\"Redis事务和分布式锁\"><a href=\"#Redis事务和分布式锁\" class=\"headerlink\" title=\"Redis事务和分布式锁\"></a>Redis事务和分布式锁</h3><h4 id=\"Redis事务\"><a href=\"#Redis事务\" class=\"headerlink\" title=\"Redis事务\"></a>Redis事务</h4><p>核心操作：multi开启事务，exec执行事务，discard撤销事务，watch监控key是否改过、UNWATCH对应watch操作【multi和exec执行后会自动unwatch调监控队列里的所有key，如果想提前释放，可以用这个命令】</p>\n<p>redis事务采用的是乐观锁，就是先处理，再真正去执行的时候去看有没有被修改过，没有的话，就去更新，否则报错。这个思想是不是有点似曾相识，是的，concurrenthashmap的set值时也是用的这个思想。还是那句话，虽然技术上感觉不搭边，但是思想上确实想通的。</p>\n<h4 id=\"Redis分布式锁\"><a href=\"#Redis分布式锁\" class=\"headerlink\" title=\"Redis分布式锁\"></a>Redis分布式锁</h4><p>核心操作：setNX，理解这个操作基本上就理解了分布式锁。这个方法的意思就是去给一个key“赋值”，如果之前没人这样做过，返回值是1，否则返回0.</p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/Jason-Xiang/p/5364252.html\" target=\"_blank\" rel=\"noopener\">Redis事务和分布式锁</a></p>\n<h2 id=\"Redis的缺点\"><a href=\"#Redis的缺点\" class=\"headerlink\" title=\"Redis的缺点\"></a>Redis的缺点</h2><p>Redis会是完美的程序吗？肯定不可能的，世界上就没有完美的东西。我们需要做的事情，分析清楚事物的主要矛盾，在实际开发中，不要犯这些错误就好了。</p>\n<p>以下典型问题，重点参考了<a href=\"https://www.cnblogs.com/rjzheng/p/9096228.html\" target=\"_blank\" rel=\"noopener\">分布式之redis复习精讲</a></p>\n<h3 id=\"缓存和数据库双写一致性问题\"><a href=\"#缓存和数据库双写一致性问题\" class=\"headerlink\" title=\"缓存和数据库双写一致性问题\"></a>缓存和数据库双写一致性问题</h3><h4 id=\"问题说明\"><a href=\"#问题说明\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。</p>\n<h4 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>详细解决方案见<a href=\"https://www.cnblogs.com/rjzheng/p/9041659.html\" target=\"_blank\" rel=\"noopener\">分布式之数据库和缓存双写一致性方案解析</a></p>\n<h3 id=\"缓存雪崩问题\"><a href=\"#缓存雪崩问题\" class=\"headerlink\" title=\"缓存雪崩问题\"></a>缓存雪崩问题</h3><p>缓存击穿问题和缓存雪崩问题都是大项目中可能会遇到，小项目比较难遇到。</p>\n<h4 id=\"问题说明-1\"><a href=\"#问题说明-1\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常</p>\n<h4 id=\"解决方案-1\"><a href=\"#解决方案-1\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试<br>2) 采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。<br>3) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。</p>\n<h3 id=\"缓存击穿问题\"><a href=\"#缓存击穿问题\" class=\"headerlink\" title=\"缓存击穿问题\"></a>缓存击穿问题</h3><h4 id=\"问题说明-2\"><a href=\"#问题说明-2\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常</p>\n<h4 id=\"解决方案-2\"><a href=\"#解决方案-2\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 给缓存的失效时间，加上一个随机值，避免集体失效。<br>2) 使用互斥锁，但是该方案吞吐量明显下降了。<br>3) 双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点</p>\n<ul>\n<li>I 从缓存A读数据库，有则直接返回</li>\n<li>II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。</li>\n<li>III 更新线程同时更新缓存A和缓存B。</li>\n</ul>\n<h3 id=\"缓存的并发竞争问题\"><a href=\"#缓存的并发竞争问题\" class=\"headerlink\" title=\"缓存的并发竞争问题\"></a>缓存的并发竞争问题</h3><h4 id=\"问题说明-3\"><a href=\"#问题说明-3\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。</p>\n<h4 id=\"解决方案-3\"><a href=\"#解决方案-3\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 如果对这个key操作，不要求顺序<br>这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。<br>2) 如果对这个key操作，要求顺序<br>假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.<br>期望按照key1的value值按照 valueA–&gt;valueB–&gt;valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳</p>\n","site":{"data":{}},"excerpt":"<p> Redis是当前比较热门的NOSQL系统之一，它是一个开源的使用ANSI c语言编写的key-value存储系统（区别于MySQL的二维表格的形式存储。）。和Memcache类似，但很大程度补偿了Memcache的不足。和Memcache一样，Redis数据都是缓存在计算机内存中，不同的是，Memcache只能将数据缓存到内存中，无法自动定期写入硬盘，这就表示，一断电或重启，内存清空，数据丢失。所以Memcache的应用场景适用于缓存无需持久化的数据。而Redis不同的是它会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，实现数据的持久化.<br>","more":"</p>\n<h2 id=\"Redis的机制\"><a href=\"#Redis的机制\" class=\"headerlink\" title=\"Redis的机制\"></a>Redis的机制</h2><h3 id=\"高性能\"><a href=\"#高性能\" class=\"headerlink\" title=\"高性能\"></a>高性能</h3><p>Redis最牛逼的地方就是快，为什么这么快呢，主要原因如下</p>\n<ul>\n<li>纯内存操作</li>\n<li>数据结构简单，对数据操作也简单</li>\n<li><p>单线程操作，避免了频繁的上下文切换<br>  单线程好处：1.代码更清晰，处理逻辑更简单；2.不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；3.不存在多进程或者多线程导致的切换而消耗CPU<br>  单线程劣势：无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善；</p>\n</li>\n<li><p>采用了<a href=\"https://blog.csdn.net/happy_wu/article/details/80052617\" target=\"_blank\" rel=\"noopener\">非阻塞I/O多路复用机制</a><br>  IO多路复用是需要操作系统支持的，目前，提供的多路复用函数库主要有四下个方法<a href=\"https://www.cnblogs.com/skiler/p/6852493.html\" target=\"_blank\" rel=\"noopener\">select、poll、epoll、kqueue等</a><br>  名词比较绕口，理解涵义就好。从网上抄来的一个epoll场景：<br>  一个酒吧服务员（一个线程），前面趴了一群醉汉，突然一个吼一声“倒酒”（事件），你小跑过去给他倒一杯，然后随他去吧，突然又一个要倒酒，你又过去倒上，就这样一个服务员服务好多人，有时没人喝酒，服务员处于空闲状态，可以干点别的玩玩手机。至于epoll与select，poll的区别在于后两者的场景中醉汉不说话，你要挨个问要不要酒，没时间玩手机了。epoll是linux下内核支持的函数，而kqueue是其他系统支持的函数。io多路复用大概就是指这几个醉汉共用一个服务员。</p>\n</li>\n</ul>\n<h3 id=\"高可用性\"><a href=\"#高可用性\" class=\"headerlink\" title=\"高可用性\"></a>高可用性</h3><h6 id=\"了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\"><a href=\"#了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\" class=\"headerlink\" title=\"了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。\"></a>了解一个技术的时候，一定要自己不断的去思考，为什么会出现这个技术，这个技术解决了什么问题，以及怎样解决的。</h6><p>Redis还有一个很牛逼的特性就是高可用性，Redis是如何做到的呢，主要原因如下</p>\n<ul>\n<li>支持持久化机制，用于解决机器故障导致内存数据丢失的问题</li>\n<li>支持主从复制，用于解决读取数据压力大问题</li>\n<li>支持哨兵机制，能够及时做到故障转移</li>\n<li>支持集群机制，能够扩充Redis可以存放的数据量</li>\n<li>支持过期策略以及内存淘汰机制，解决写入数据高于Redis总内存问题</li>\n<li>事务控制和分布式锁，保证并发场景下正常的使用Redis</li>\n</ul>\n<p>下面依次分析这几个机制</p>\n<h3 id=\"持久化机制\"><a href=\"#持久化机制\" class=\"headerlink\" title=\"持久化机制\"></a>持久化机制</h3><h4 id=\"持久化作用\"><a href=\"#持久化作用\" class=\"headerlink\" title=\"持久化作用\"></a>持久化作用</h4><p>Redis持久化就是按照一定的机制及时将内存里的数据同步至磁盘。</p>\n<h4 id=\"为什么要去支持持久化呢？\"><a href=\"#为什么要去支持持久化呢？\" class=\"headerlink\" title=\"为什么要去支持持久化呢？\"></a>为什么要去支持持久化呢？</h4><p>Redis之前比较流行的内存数据库是memcached，由于它的数据都存储到内存里了，就导致它有一个很致命的缺陷，就是一旦机制出现故障，所有数据就会丢失，这一点就一定决定了memcached的应用场景会非常受限，只能应用于一些非核心业务，可以接受数据丢失。</p>\n<h4 id=\"持久化机制-1\"><a href=\"#持久化机制-1\" class=\"headerlink\" title=\"持久化机制\"></a>持久化机制</h4><ul>\n<li>RDB<br>可以在指定的时间间隔内生成数据集的时间点快照</li>\n<li>AOF<br>持久化记录服务器执行的所有写操作命令</li>\n<li>同时启用AOF和RDB<br>当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。为了缓解这一问题，Redis支持Rewrite重写，简单的讲就是基于一定算法，给Aof文件“瘦身”。</li>\n</ul>\n<p>优缺点分析</p>\n<ul>\n<li><p>RDB Redis会单独启动一个进程，先把数据收集到一个临时文件，等待上一个持久化操作结束后，并且按照你配置的持久策略去把数据同步到磁盘，对主进程无影响，性能高，但是缺点是，一旦出现机器故障，放在缓存里的数据就会丢失。<br>RDB策略配置示例如下（redis.conf）<br>每900s有一次写操作触发同步，或者每300s有10次写操作触发同步，或者每60s有10000次操作触发同步</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">save 900 1</span><br><span class=\"line\">save 300 10</span><br><span class=\"line\">save 60 10000</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>AOF Redis会将自己所有执行过的更新指令记录下来，并且日志文件只允许尾部添加操作。恢复数据的时候，Redis会从头到尾重新执行一遍。<br>Aof目前同步策略：Always（每步）Everysec（每一秒）No（操作系统决定）。<br>开启aof配置，以及同步策略配置示例如下（redis.conf）<br>开启aof，每秒同步一次写操作到磁盘</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">appendonly yes </span><br><span class=\"line\">appendfsync everysec</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>这两种机制没有好坏之说，使用的时候，需要你去分析自己的实际业务场景，哪个适合用哪个！<br>参考链接：<a href=\"https://www.cnblogs.com/shsxt/p/7911591.html\" target=\"_blank\" rel=\"noopener\">Redis持久化</a></p>\n<h3 id=\"主从复制\"><a href=\"#主从复制\" class=\"headerlink\" title=\"主从复制\"></a>主从复制</h3><h4 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>Redis相对于传统的db，读写非常快的，但是，面对更大的读请求来临时，Redis提供了主从复制策略来解决读取数据压力大问题</p>\n<h4 id=\"结构\"><a href=\"#结构\" class=\"headerlink\" title=\"结构\"></a>结构</h4><p>Redis 主从master-slave级联图</p>\n<p><img src=\"https://li-shengming.github.io/pictures/redis/redis-master-slave.png\" alt=\"redis-master-slave\"></p>\n<p><strong>主从同步策略</strong></p>\n<p>主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。<br>开启从服务器配置（redis.conf）<br>将当前服务器配置成192.168.1.1 6379 这个节点的从服务器<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">slaveof 192.168.1.1 6379</span><br></pre></td></tr></table></figure></p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/hepingqingfeng/p/7263782.html\" target=\"_blank\" rel=\"noopener\">主从复制</a></p>\n<h3 id=\"哨兵\"><a href=\"#哨兵\" class=\"headerlink\" title=\"哨兵\"></a>哨兵</h3><h3 id=\"作用-1\"><a href=\"#作用-1\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>如果Redis主节点跪了的话，如何来保证Redis系统可用性呢？Redis提供了哨兵机制，哨兵会去监控Redis节点，当发现Redis主节点跪了的时候，会通过投票机制，从从节点中挑取一个节点自动升级为主节点，自动实现故障转移，同事还会向管理员发送Redis节点异常通知。</p>\n<h4 id=\"结构-1\"><a href=\"#结构-1\" class=\"headerlink\" title=\"结构\"></a>结构</h4><p>Redis 哨兵监控图(哨兵也可以多个)<br><img src=\"https://li-shengming.github.io/pictures/redis/redis-sentinel.png\" alt=\"redis-master-slave\"><br><strong>哨兵原理介绍</strong></p>\n<p>首先理解两个名词</p>\n<ul>\n<li>主观下线：当前哨兵节点连接某一Redis节点失败；</li>\n<li><p>客观下线：当sentinel监视的某个服务主观下线后，sentinel会询问其它监视该服务的sentinel，看它们是否也认为该服务主观下线，接收到足够数量（这个值可以配置）的sentinel判断为主观下线，既任务该服务客观下线，并对其做故障转移操作。<br>选举领头哨兵</p>\n<p>一个redis服务被判断为客观下线时，多个监视该服务的sentinel协商，选举一个领头sentinel，对该redis服务进行古战转移操作。选举领头sentinel会遵循一些规则，有兴趣可以自行调研。<br>  注：你可能会关心为啥选举领头哨兵，其实道理很简单，做故障迁移的时候，不可能每个哨兵节点都私自决定用哪个节点替代坏掉的主节点，<br>  会乱套的，公平的投出一个节点作为领头者，然后基于一定规则去做故障转移最快<br>进行故障转移，分为三个主要步骤：<br>1) 从下线的主服务的所有从服务里面挑选一个从服务，将其转成主服务；<br>2) 已下线主服务的所有从服务改为复制新的主服务；<br>3) 将已下线的主服务设置成新的主服务的从服务，当其回复正常时，复制新的主服务，变成新的主服务的从服务。<br>参考链接：<a href=\"https://blog.csdn.net/RobertoHuang/article/details/70768922\" target=\"_blank\" rel=\"noopener\">Redis哨兵</a></p>\n<h3 id=\"集群\"><a href=\"#集群\" class=\"headerlink\" title=\"集群\"></a>集群</h3><h4 id=\"作用-2\"><a href=\"#作用-2\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>一台Redis节点内存资源总是有限的，如果不够使的话，怎么能扩展吗？<br>Redis是支持集群的，默认关闭。<br>开启集群配置(redis.conf)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster-enabled yes #开启cluster</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"Redis集群架构\"><a href=\"#Redis集群架构\" class=\"headerlink\" title=\"Redis集群架构\"></a>Redis集群架构</h4><p><img src=\"https://li-shengming.github.io/pictures/redis/redis-cluster.png\" alt=\"redis-cluster\"><br>图上能看到的信息：<br>1) 对象保存到Redis之前先经过CRC16哈希到一个指定的Node上，例如Object4最终Hash到了Node1上。<br>2) 每个Node被平均分配了一个Slot段，对应着0-16384，Slot不能重复也不能缺失，否则会导致对象重复存储或无法存储。<br>3) Node之间也互相监听，一旦有Node退出或者加入，会按照Slot为单位做数据的迁移。例如Node1如果掉线了，0-5640这些Slot将会平均分摊到Node2和Node3上,由于Node2和Node3本身维护的Slot还会在自己身上不会被重新分配，所以迁移过程中不会影响到5641-16384Slot段的使用。</p>\n<p>简单总结下哈希Slot的优缺点：</p>\n<ul>\n<li>优点：将Redis的写操作分摊到了多个节点上，提高写的并发能力，扩容简单。</li>\n<li>缺点：每个Node承担着互相监听、高并发数据写入、高并发数据读出，工作任务繁重</li>\n</ul>\n<p>延展性问题：redis为什么采用一致性hash，而没有采用传统的取模算法？<br>其实道理很简答，因为在集群环境下，新增节点或者节点失效是很常见的，如果采用取模的那种算法，每次新增或者删除节点时，所有与数据都可能需要重新调整位置，这是无法接受的！<br>结合redis集群和主从复制两种思想，可以得到Redis集群的最终形态</p>\n<p><img src=\"https://li-shengming.github.io/pictures/redis/redis-cluster-final.png\" alt=\"redis-cluster-final\"></p>\n<p>想扩展并发读就添加Slaver，想扩展并发写就添加Master，想扩容也就是添加Master，任何一个Slaver或者几个Master挂了都不会是灾难性的故障。完美！<br>参考链接：<a href=\"https://blog.csdn.net/yejingtao703/article/details/78484151\" target=\"_blank\" rel=\"noopener\">Redis集群设计原理</a></p>\n<h3 id=\"内存数据管理方案\"><a href=\"#内存数据管理方案\" class=\"headerlink\" title=\"内存数据管理方案\"></a>内存数据管理方案</h3><h4 id=\"作用-3\"><a href=\"#作用-3\" class=\"headerlink\" title=\"作用\"></a>作用</h4><p>这一部分对应的问题很常见，就是如果你的redis只能存5g数据，可是你写入了10g数据，那么内存里最终会保留哪5g数据的问题。<br>redis采用的是定期删除+惰性删除策略。</p>\n<p><strong>为什么不用定时删除策略?</strong><br>定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.</p>\n<p><strong>定期删除+惰性删除是如何工作的呢?</strong><br>定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。<br>于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。</p>\n<p><strong>采用定期删除+惰性删除就没其他问题了么?</strong><br>不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。<br>在redis.conf中有一行配置示例(当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">maxmemory-policy volatile-lru</span><br></pre></td></tr></table></figure></p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/rjzheng/p/9096228.html\" target=\"_blank\" rel=\"noopener\">分布式之redis复习精讲</a></p>\n<h3 id=\"Redis事务和分布式锁\"><a href=\"#Redis事务和分布式锁\" class=\"headerlink\" title=\"Redis事务和分布式锁\"></a>Redis事务和分布式锁</h3><h4 id=\"Redis事务\"><a href=\"#Redis事务\" class=\"headerlink\" title=\"Redis事务\"></a>Redis事务</h4><p>核心操作：multi开启事务，exec执行事务，discard撤销事务，watch监控key是否改过、UNWATCH对应watch操作【multi和exec执行后会自动unwatch调监控队列里的所有key，如果想提前释放，可以用这个命令】</p>\n<p>redis事务采用的是乐观锁，就是先处理，再真正去执行的时候去看有没有被修改过，没有的话，就去更新，否则报错。这个思想是不是有点似曾相识，是的，concurrenthashmap的set值时也是用的这个思想。还是那句话，虽然技术上感觉不搭边，但是思想上确实想通的。</p>\n<h4 id=\"Redis分布式锁\"><a href=\"#Redis分布式锁\" class=\"headerlink\" title=\"Redis分布式锁\"></a>Redis分布式锁</h4><p>核心操作：setNX，理解这个操作基本上就理解了分布式锁。这个方法的意思就是去给一个key“赋值”，如果之前没人这样做过，返回值是1，否则返回0.</p>\n<p>参考链接：<a href=\"https://www.cnblogs.com/Jason-Xiang/p/5364252.html\" target=\"_blank\" rel=\"noopener\">Redis事务和分布式锁</a></p>\n<h2 id=\"Redis的缺点\"><a href=\"#Redis的缺点\" class=\"headerlink\" title=\"Redis的缺点\"></a>Redis的缺点</h2><p>Redis会是完美的程序吗？肯定不可能的，世界上就没有完美的东西。我们需要做的事情，分析清楚事物的主要矛盾，在实际开发中，不要犯这些错误就好了。</p>\n<p>以下典型问题，重点参考了<a href=\"https://www.cnblogs.com/rjzheng/p/9096228.html\" target=\"_blank\" rel=\"noopener\">分布式之redis复习精讲</a></p>\n<h3 id=\"缓存和数据库双写一致性问题\"><a href=\"#缓存和数据库双写一致性问题\" class=\"headerlink\" title=\"缓存和数据库双写一致性问题\"></a>缓存和数据库双写一致性问题</h3><h4 id=\"问题说明\"><a href=\"#问题说明\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。</p>\n<h4 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>详细解决方案见<a href=\"https://www.cnblogs.com/rjzheng/p/9041659.html\" target=\"_blank\" rel=\"noopener\">分布式之数据库和缓存双写一致性方案解析</a></p>\n<h3 id=\"缓存雪崩问题\"><a href=\"#缓存雪崩问题\" class=\"headerlink\" title=\"缓存雪崩问题\"></a>缓存雪崩问题</h3><p>缓存击穿问题和缓存雪崩问题都是大项目中可能会遇到，小项目比较难遇到。</p>\n<h4 id=\"问题说明-1\"><a href=\"#问题说明-1\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常</p>\n<h4 id=\"解决方案-1\"><a href=\"#解决方案-1\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试<br>2) 采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。<br>3) 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。</p>\n<h3 id=\"缓存击穿问题\"><a href=\"#缓存击穿问题\" class=\"headerlink\" title=\"缓存击穿问题\"></a>缓存击穿问题</h3><h4 id=\"问题说明-2\"><a href=\"#问题说明-2\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常</p>\n<h4 id=\"解决方案-2\"><a href=\"#解决方案-2\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 给缓存的失效时间，加上一个随机值，避免集体失效。<br>2) 使用互斥锁，但是该方案吞吐量明显下降了。<br>3) 双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点</p>\n<ul>\n<li>I 从缓存A读数据库，有则直接返回</li>\n<li>II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。</li>\n<li>III 更新线程同时更新缓存A和缓存B。</li>\n</ul>\n<h3 id=\"缓存的并发竞争问题\"><a href=\"#缓存的并发竞争问题\" class=\"headerlink\" title=\"缓存的并发竞争问题\"></a>缓存的并发竞争问题</h3><h4 id=\"问题说明-3\"><a href=\"#问题说明-3\" class=\"headerlink\" title=\"问题说明\"></a>问题说明</h4><p>这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。</p>\n<h4 id=\"解决方案-3\"><a href=\"#解决方案-3\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>1) 如果对这个key操作，不要求顺序<br>这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。<br>2) 如果对这个key操作，要求顺序<br>假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.<br>期望按照key1的value值按照 valueA–&gt;valueB–&gt;valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjv7tymt30002c0ud7wntg3ve","category_id":"cjv7tymt50003c0udmr4w56c8","_id":"cjv7tymt70006c0udi54py5rf"},{"post_id":"cjv7tynd40009c0udcn22i0zm","category_id":"cjv7tynd5000ac0udh7vqpsb9","_id":"cjv7tynd6000ec0ud24qix4bc"}],"PostTag":[{"post_id":"cjv7tymt30002c0ud7wntg3ve","tag_id":"cjv7tymt60004c0udwrzji43x","_id":"cjv7tymt80007c0udr9tw5bzf"},{"post_id":"cjv7tymt30002c0ud7wntg3ve","tag_id":"cjv7tymt70005c0udu2viowzf","_id":"cjv7tymt80008c0udqwmjh76b"},{"post_id":"cjv7tynd40009c0udcn22i0zm","tag_id":"cjv7tynd5000bc0ud6w12cit0","_id":"cjv7tynd6000cc0udpfufi57y"},{"post_id":"cjv7tynd40009c0udcn22i0zm","tag_id":"cjv7tymt70005c0udu2viowzf","_id":"cjv7tynd6000dc0udf90jw07t"}],"Tag":[{"name":"Eureka","_id":"cjv7tymt60004c0udwrzji43x"},{"name":"SourceCode","_id":"cjv7tymt70005c0udu2viowzf"},{"name":"Redis","_id":"cjv7tynd5000bc0ud6w12cit0"}]}}